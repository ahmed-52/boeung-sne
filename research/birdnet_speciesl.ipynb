{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8050dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "import librosa\n",
    "import birdnetlib\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "from pprint import pprint \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d492b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from birdnetlib import Recording\n",
    "from birdnetlib.analyzer import Analyzer\n",
    "from birdnetlib.batch import DirectoryMultiProcessingAnalyzer\n",
    "from birdnetlib.species import SpeciesList\n",
    "from birdnetlib.watcher import DirectoryWatcher\n",
    "from datetime import datetime \n",
    "import re \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4c095ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_directory(dir_path):\n",
    "    if os.path.exists(dir_path):\n",
    "        shutil.rmtree(dir_path) # remove every file in a given directory\n",
    "    os.mkdir(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3382ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_date(filename):\n",
    "    pattern = r\"(\\d{8})_(\\d{6})\" # search for pattern in file name of year/month/day_hour/minute/second\n",
    "    match = re.search(pattern, filename)\n",
    "    date, time = match.groups()\n",
    "    return datetime.strptime(date + time, \"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e27a0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_file_detections(recording):\n",
    "    print(\"\\n\" + recording.path)\n",
    "    pprint(recording.date)\n",
    "    print(\"predicted species: \\n\")\n",
    "    species_pred = SpeciesList() # lat and lon values taken from spreadsheet with arus \n",
    "    species_predicted = species_pred.return_list(lon=105.398278, lat=11.403694, \n",
    "                                                    date=recording.date, threshold=0.75) \n",
    "    pprint(species_predicted)\n",
    "    print(\"detected species: \\n\")\n",
    "    pprint(recording.detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "885f132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_file():\n",
    "    \n",
    "    analysers = {\n",
    "        \"default\" : Analyzer(),\n",
    "        \"full_species\" : Analyzer(custom_species_list=\"species_lists/full_species_list.txt\"),\n",
    "        \"main_species\" : Analyzer(custom_species_list=\"species_lists/main_species_list.txt\"),\n",
    "    }\n",
    "    \n",
    "    analyser_colours = {\n",
    "        \"default\" : \"black\",\n",
    "        \"full_species\" : \"red\", \n",
    "        \"main_species\" : \"cyan\",\n",
    "    }\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    export_directory = f\"extractions/high_conf_{timestamp}\"\n",
    "    os.makedirs(export_directory, exist_ok=True) # set exist_ok = true to suppress error even if directory already exists \n",
    "    \n",
    "    csv_path = os.path.join(export_directory, f\"detections_{timestamp}.csv\") # put csv of detections in same directory as corresponding extractions\n",
    "    csv_headers = [\n",
    "        \"file_name\", \"best_analyzer\", \"common_name\", \"scientific_name\",\n",
    "        \"confidence\", \"start_time\", \"end_time\", \"audio_path\",\n",
    "        \"spectrogram_path\", \"lat\", \"lon\", \"date\"\n",
    "    ]\n",
    "    \n",
    "    with open(csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file: # w = write mode \n",
    "        writer = csv.DictWriter(csv_file, fieldnames=csv_headers)\n",
    "        writer.writeheader() # writes header row to csv\n",
    "    \n",
    "        file_directory = \"aru\"\n",
    "        \n",
    "        lat, lon = 11.403694, 105.398278\n",
    "        \n",
    "        file_paths = [os.path.join(file_directory, f) for f in os.listdir(file_directory)\n",
    "            if f.lower().endswith((\".wav\", \".mp3\"))] # store full path for every file in aru directory \n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            file_date = extract_file_date(file_name)\n",
    "            print(f\"\\nanalysing {file_name}\")\n",
    "            \n",
    "            file_detections = [] # association list of all detections in a file with what analyser they used \n",
    "            \n",
    "            for analyser_name, analyser in analysers.items():\n",
    "                try: \n",
    "                    if analyser.has_custom_species_list: # analyser with custom species list cannot use lat, lon\n",
    "                        recording = Recording(\n",
    "                            analyzer=analyser, \n",
    "                            path=file_path,\n",
    "                            date=file_date,\n",
    "                            min_conf=0.9,\n",
    "                            return_all_detections=True\n",
    "                        )\n",
    "                    else:\n",
    "                        recording = Recording(\n",
    "                            analyzer=analyser, \n",
    "                            path=file_path,\n",
    "                            date=file_date,\n",
    "                            lat=lat,\n",
    "                            lon=lon, \n",
    "                            min_conf=0.9,\n",
    "                            return_all_detections=True\n",
    "                        )\n",
    "                    \n",
    "                    recording.analyze() \n",
    "                    for detection in recording.detections:\n",
    "                        detection[\"analyser\"] = analyser_name\n",
    "                        detection[\"recording\"] = recording\n",
    "                        file_detections.append(detection)\n",
    "                except Exception as e:\n",
    "                    print(f\"analysis of {file_name} using {analyser_name} failed with: {e}\")\n",
    "                    \n",
    "            best_file_detections = {}\n",
    "            for detection in file_detections:\n",
    "                species_name = detection.get(\"common_name\")\n",
    "                species_detected = (species_name, round(detection.get(\"start_time\"), 2)) # round detection start time to 2 decimal places\n",
    "                if (species_detected not in best_file_detections) or (detection[\"confidence\"] > best_file_detections[species_detected][\"confidence\"]):\n",
    "                    best_file_detections[species_detected] = detection # if this detection has the highest confidence value for this snippet, store this\n",
    "                    \n",
    "            for species in best_file_detections.keys():\n",
    "                detection = best_file_detections[species]\n",
    "                if detection[\"analyser\"] == \"default\":\n",
    "                    species_name = detection.get(\"common_name\")\n",
    "                    for specific_analyser in [\"main_species\", \"full_species\"]: # main > full > default in priority (of specialisation)\n",
    "                        for detect in file_detections:\n",
    "                            if (detect[\"analyser\"] == specific_analyser) and (detect.get(\"common_name\") == species_name):\n",
    "                                best_file_detections[species] = detect # if there is a detection in main then use that, if not if there is one in full use that, otherwise stay with default\n",
    "                                break     \n",
    "                \n",
    "            for detection in best_file_detections.values(): # extracting (species, start time) for highest confidence clips\n",
    "                recording = detection[\"recording\"]\n",
    "                species_name = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", detection.get(\"common_name\")) # if the common name has weird things in it, replace with _ \n",
    "                \n",
    "                species_directory = os.path.join(export_directory, species_name)\n",
    "                os.makedirs(species_directory, exist_ok=True) # make directories to store species detections by file name \n",
    "                \n",
    "                try:\n",
    "                    recording.extract_detections_as_audio(directory=species_directory, format=\"mp3\", min_conf=0.9, padding_secs=2)\n",
    "                    recording.extract_detections_as_spectrogram(directory=species_directory, min_conf=0.9, padding_secs=2)\n",
    "                    \n",
    "                    writer.writerow({\n",
    "                        \"file_name\" : file_name, \n",
    "                        \"best_analyzer\" : detection[\"analyser\"], \n",
    "                        \"common_name\" : detection[\"common_name\"], \n",
    "                        \"scientific_name\" : detection[\"scientific_name\"],\n",
    "                        \"confidence\" : detection[\"confidence\"], \n",
    "                        \"start_time\" : detection[\"start_time\"], \n",
    "                        \"end_time\" : detection[\"end_time\"], \n",
    "                        \"audio_path\" : species_directory,\n",
    "                        \"spectrogram_path\" : species_directory, \n",
    "                        \"lat\" : lat, \n",
    "                        \"lon\" : lon, \n",
    "                        \"date\" : file_date.strftime(\"%Y-%m-%d\"), \n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"extraction of {file_name} using {analyser_name} failed with: {e}\")\n",
    "                    \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82cb4b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels loaded.\n",
      "load model True\n",
      "Model loaded.\n",
      "Labels loaded.\n",
      "load_species_list_model\n",
      "Meta model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anku2024/Documents/clink lab/boeung-sne/birdc/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels loaded.\n",
      "load model True\n",
      "Model loaded.\n",
      "Labels loaded.\n",
      "load_species_list_model\n",
      "Meta model loaded.\n",
      "Labels loaded.\n",
      "load model True\n",
      "Model loaded.\n",
      "Labels loaded.\n",
      "load_species_list_model\n",
      "Meta model loaded.\n",
      "\n",
      "analysing 3_S7901_20250204_070000(UTC+7).wav\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 3_S7901_20250204_070000(UTC+7).wav\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "return_predicted_species_list\n",
      "5\n",
      "511 species loaded.\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 3_S7901_20250204_070000(UTC+7).wav\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 3_S7901_20250204_070000(UTC+7).wav\n"
     ]
    }
   ],
   "source": [
    "analyse_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
